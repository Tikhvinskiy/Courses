{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72ef5139",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9e8d1e7f450f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmore_itertools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import PIL\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import transformers\n",
    "import more_itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass, field\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from tqdm.contrib import tzip\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    pils = imgs\n",
    "    \n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid\n",
    "\n",
    "def read_video(path, transform=None, frames_num=16, window=30):\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    \n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    \n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    N = length//(frames_num)\n",
    "    #print(length)\n",
    "    #counter = \n",
    "    \n",
    "    current_frame = 0\n",
    "    for i in range(length):\n",
    "    \n",
    "        #frameId = int(round(cap.get(current_frame))) \n",
    "        #print(current_frame)\n",
    "        ret, frame = cap.read(current_frame)\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if ret and i==current_frame and len(frames)<frames_num:\n",
    "            size = 196, 196\n",
    "            frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            frame.thumbnail(size, Image.ANTIALIAS)\n",
    "            \n",
    "            frames.append(frame)\n",
    "            current_frame += N\n",
    "        \n",
    "       \n",
    "        #print(current_frame)\n",
    "        #cap.set(cv2.CAP_PROP_POS_FRAMES, current_frame)\n",
    "        \n",
    "        \n",
    "    cap.release()\n",
    "    #print(frames)\n",
    "    return frames\n",
    "device = torch.device('cpu')\n",
    "clip_model_type = \"ViT-L/14@336px\"\n",
    "\n",
    "out_path = f\"Features_train.pkl\"\n",
    "video_path =  'videos'\n",
    "path_a = 'train.csv'\n",
    "\n",
    "\n",
    "clip_model, preprocess = clip.load(clip_model_type, device=device, jit=False)\n",
    "df = pd.read_csv(path_a)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_embeddings = []\n",
    "all_captions = []\n",
    "i = 0\n",
    "for caption, video_name in tzip(df.caption, df.paths):\n",
    "    name = f'{video_path}/{video_name}'\n",
    "    text = f'Caption: {caption}<|endoftext|>'\n",
    "    #print(name)\n",
    "    if os.path.exists(name):\n",
    "        \n",
    "        video = read_video(path = name, frames_num=4)\n",
    "        if len(video)>1:\n",
    "            #print(len(video))\n",
    "            image = image_grid(video,2,2)\n",
    "\n",
    "            image = preprocess(image).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                prefix = clip_model.encode_image(image).cpu()\n",
    "            #d[\"clip_embedding\"] = i\n",
    "            all_embeddings.append(prefix)\n",
    "            all_captions.append(text)\n",
    "    \n",
    "with open(out_path, 'wb') as f:\n",
    "    pickle.dump({\"clip_embedding\": torch.cat(all_embeddings, dim=0), \"captions\": all_captions}, f)\n",
    "\n",
    "print('Done')\n",
    "print(\"%0d embeddings saved \" % len(all_embeddings))\n",
    "\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "import gc\n",
    "import io\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torchvision\n",
    "import transformers\n",
    "import more_itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass, field\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import sys\n",
    "from tqdm.contrib import tzip\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as nnf\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import argparse\n",
    "import json\n",
    "from typing import Tuple, Optional, Union\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "import wandb\n",
    "import torch\n",
    "\n",
    "from torch.utils.checkpoint import checkpoint_sequential\n",
    "\n",
    "class ClipCocoDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_path: str,  prefix_length= 50, gpt2_type = \"gpt2\",\n",
    "                 normalize_prefix=False):\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        self.prefix_length = prefix_length\n",
    "        self.normalize_prefix = normalize_prefix\n",
    "        with open(data_path, 'rb') as f:\n",
    "            all_data = pickle.load(f)\n",
    "        print(\"Data size is %0d\" % len(all_data[\"clip_embedding\"]))\n",
    "        sys.stdout.flush()\n",
    "        self.prefixes = all_data[\"clip_embedding\"]\n",
    "        captions_raw = all_data[\"captions\"]\n",
    "        \n",
    "        #self.image_ids = [caption[\"image_id\"] for caption in captions_raw]\n",
    "        \n",
    "        self.captions = captions_raw\n",
    "        \n",
    "        \n",
    "        self.captions_tokens = []\n",
    "        self.caption2embedding = []\n",
    "        max_seq_len = 0\n",
    "        i=0\n",
    "        for caption in tqdm(captions_raw):\n",
    "                self.captions_tokens.append(torch.tensor(self.tokenizer.encode(caption), dtype=torch.int64))\n",
    "                self.caption2embedding.append(self.prefixes[i])\n",
    "                i+=1\n",
    "                max_seq_len = max(max_seq_len, self.captions_tokens[-1].shape[0])\n",
    "            # self.max_seq_len = max_seq_len\n",
    "        #del self.captions_tokens\n",
    "        #del self.caption2embedding\n",
    "        #gc.collect()\n",
    "        #with open(f\"{data_path[:-4]}_tokens.pkl\", 'wb') as f:\n",
    "        #        pickle.dump([self.captions_tokens, self.caption2embedding, max_seq_len], f)\n",
    "       \n",
    "    \n",
    "    \n",
    "        all_len = torch.tensor([len(self.captions_tokens[i]) for i in range(len(self))]).float()\n",
    "        self.max_seq_len = min(int(all_len.mean() + all_len.std() * 10), int(all_len.max()))\n",
    "\n",
    "    def pad_tokens(self, item: int):\n",
    "        tokens = self.captions_tokens[item]\n",
    "        padding = self.max_seq_len - tokens.shape[0]\n",
    "        if padding > 0:\n",
    "            tokens = torch.cat((tokens, torch.zeros(padding, dtype=torch.int64) - 1))\n",
    "            self.captions_tokens[item] = tokens\n",
    "        elif padding < 0:\n",
    "            tokens = tokens[:self.max_seq_len]\n",
    "            self.captions_tokens[item] = tokens\n",
    "        mask = tokens.ge(0)  # mask is zero where we out of sequence\n",
    "        tokens[~mask] = 0\n",
    "        mask = mask.float()\n",
    "        mask = torch.cat((torch.ones(self.prefix_length), mask), dim=0)  # adding prefix mask\n",
    "        return tokens, mask\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.captions_tokens)\n",
    "\n",
    "   \n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        tokens, mask = self.pad_tokens(item)\n",
    "        prefix = self.prefixes[item]\n",
    "        if self.normalize_prefix:\n",
    "            prefix = prefix.float()\n",
    "            prefix = prefix / prefix.norm(2, -1)\n",
    "        return tokens, mask, prefix\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "wandb.init(project=\"clip_caption_video\")\n",
    "\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(sizes) - 1):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n",
    "            if i < len(sizes) - 2:\n",
    "                layers.append(act())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    #@autocast()  \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "    \n",
    "def freeze(\n",
    "    model,\n",
    "    freeze_emb=False,\n",
    "    freeze_ln=False,\n",
    "    freeze_attn=True,\n",
    "    freeze_ff=True,\n",
    "    freeze_other=True,\n",
    "):\n",
    "    \n",
    "    for name, p in model.named_parameters():\n",
    "    # freeze all parameters except the layernorm and positional embeddings\n",
    "       \n",
    "       \n",
    "        \n",
    "        name = name.lower()\n",
    "        if 'ln' in name or 'norm' in name:\n",
    "            p.requires_grad = not freeze_ln\n",
    "        elif 'embeddings' in name:\n",
    "            p.requires_grad = not freeze_emb\n",
    "        elif 'mlp' in name:\n",
    "            p.requires_grad = not freeze_ff\n",
    "        elif 'attn' in name:\n",
    "            p.requires_grad = not freeze_attn\n",
    "        else:\n",
    "            p.requires_grad = not freeze_other\n",
    "           \n",
    "    return model\n",
    "\n",
    "class ClipCaptionModel(nn.Module):\n",
    "    def __init__(self, backbone, prefix_length: int, prefix_size: int = 768):\n",
    "          super(ClipCaptionModel, self).__init__()\n",
    "          self.prefix_length = prefix_length\n",
    "          \"\"\"\n",
    "          ru gpts shit\n",
    "          \n",
    "          \"\"\"\n",
    "          self.gpt = GPT2LMHeadModel.from_pretrained(backbone)\n",
    "          #self.gpt = freeze(self.gpt)\n",
    "          self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
    "          self.clip_project = MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2,\n",
    "                                  self.gpt_embedding_size * prefix_length))\n",
    "\n",
    "    def get_dummy_token(self, batch_size: int, device: torch.device) -> torch.Tensor:\n",
    "        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n",
    "    \n",
    "    # @autocast() \n",
    "    def forward(self, tokens: torch.Tensor, prefix: torch.Tensor, mask: Optional[torch.Tensor] = None,\n",
    "                labels: Optional[torch.Tensor] = None):\n",
    "\n",
    "        embedding_text = self.gpt.transformer.wte(tokens)\n",
    "        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n",
    "\n",
    "        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n",
    "        if labels is not None:\n",
    "            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n",
    "            labels = torch.cat((dummy_token, tokens), dim=1)\n",
    "        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n",
    "        return out\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "class ClipCaptionPrefix(ClipCaptionModel):\n",
    "\n",
    "    def parameters(self, recurse: bool = True):\n",
    "        return self.clip_project.parameters()\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        super(ClipCaptionPrefix, self).train(mode)\n",
    "        self.gpt.eval()\n",
    "        return self\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(dataset: ClipCocoDataset, model: ClipCaptionModel, args,\n",
    "          warmup_steps: int = 5000, output_dir: str = \".\", output_prefix: str = \"\"):\n",
    "\n",
    "    device = torch.device('cuda')# xm.xla_device()\n",
    "    #\n",
    "    batch_size = args.bs\n",
    "    epochs = args.epochs\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    #model = freeze(model)\n",
    "    model.train()\n",
    "    optimizer = AdamW(model.parameters(),lr=args.lr,betas=(0.9, 0.995))\n",
    "    #optimizer = bnb.optim.Adam8bit(model.parameters(), lr=0.001, betas=(0.9, 0.995))\n",
    "    #optimizer = SM3(model.parameters(),lr=args.lr)\n",
    "    #Adafactor(model.parameters(),scale_parameter=True, relative_step=True, warmup_init=True, lr=None)\n",
    "\n",
    "    train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=epochs * len(train_dataloader)\n",
    "    )\n",
    "    #AdafactorSchedule(optimizer)#num_training_steps=epochs * len(train_dataloader\n",
    "    #save_config(args)\n",
    "    #print\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\">>> Training epoch {epoch}\")\n",
    "        sys.stdout.flush()\n",
    "        progress = tqdm(total=len(train_dataloader), desc=output_prefix)\n",
    "        step=0\n",
    "        for idx, (tokens, mask, prefix) in enumerate(train_dataloader):\n",
    "            model.zero_grad()\n",
    "            step+=1\n",
    "            tokens, mask, prefix = tokens.to(device), mask.to(device), prefix.to(device, dtype=torch.float32)\n",
    "            \n",
    "            outputs = model(tokens, prefix, mask)\n",
    "            logits = outputs.logits[:, dataset.prefix_length - 1: -1]\n",
    "\n",
    "            loss = nnf.cross_entropy(logits.reshape(-1, logits.shape[-1]), tokens.flatten(), ignore_index=0)\n",
    "\n",
    "            segments = 2\n",
    "\n",
    "           \n",
    "            #out = checkpoint_sequential(modules, segments, input_var)\n",
    "\n",
    "            # backpropagate\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            #optimizer.zero_grad()\n",
    "            progress.set_postfix({\"loss\": loss.item()})\n",
    "            \n",
    "            clipping_value = 0.5 # arbitrary value of your choosing\n",
    "            #torch.nn.utils.clip_grad_norm(model.parameters(), clipping_value)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            wandb.log({\"loss\":  loss.item()})\n",
    "            \n",
    "            progress.update()\n",
    "            \n",
    "\n",
    "            del tokens\n",
    "            del mask\n",
    "            del prefix\n",
    "            torch.clear_autocast_cache()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            if (idx + 1) % 7000 == 0:\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    \n",
    "                    os.path.join(output_dir, f\"{output_prefix}_latest.pt\"),\n",
    "                )\n",
    "        progress.close()\n",
    "        if epoch % args.save_every ==0:\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(output_dir, f\"{output_prefix}-{epoch:03d}.pt\"),\n",
    "            )\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.backbone = 'gpt2'\n",
    "        self.data = 'Features_train.pkl'\n",
    "        self.out_dir = 'checkpoints'\n",
    "        self.prefix = 'refix'\n",
    "        self.epochs = 10\n",
    "        self.save_every = 1\n",
    "        self.prefix_length = 40\n",
    "        self.bs = 10\n",
    "        self.only_prefix = False\n",
    "        self.lr = 5e-5\n",
    "        \n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    \n",
    "    args = Args()\n",
    "    wandb.config = {\n",
    "      \"learning_rate\": args.lr,\n",
    "      \"epochs\": args.epochs,\n",
    "      \"batch_size\": args.bs\n",
    "    }\n",
    "\n",
    "    prefix_length = args.prefix_length\n",
    "\n",
    "    dataset = ClipCocoDataset(args.data, prefix_length)\n",
    "    \n",
    "   \n",
    "    model = ClipCaptionModel(prefix_length = prefix_length, backbone = args.backbone)\n",
    "    print(\"Train both prefix and GPT\")\n",
    "    sys.stdout.flush()\n",
    "    train(dataset, model, args, output_dir=args.out_dir, output_prefix=args.prefix)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
